{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pybboxes as pbx\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = \"../data/annotations/\"\n",
    "images_path = \"../data/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = os.listdir(annotations_path)\n",
    "images = os.listdir(images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_list = []\n",
    "width_list = []\n",
    "height_list = []\n",
    "label_list = []\n",
    "x_min = []\n",
    "y_min = []\n",
    "x_max = []\n",
    "y_max = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 877/877 [00:00<00:00, 6800.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(annotations))):\n",
    "    tree = ET.parse(os.path.join(annotations_path, annotations[i]))\n",
    "    root = tree.getroot()\n",
    "    img_name = root.find(\"filename\").text\n",
    "\n",
    "    size = root.find(\"size\")\n",
    "    width = int(size.find(\"width\").text)\n",
    "    height = int(size.find(\"height\").text)\n",
    "\n",
    "    for group in root.findall(\"object\"):\n",
    "        label = group.find(\"name\").text\n",
    "        bbox = group.find(\"bndbox\")\n",
    "        xmin = int(bbox.find(\"xmin\").text)\n",
    "        ymin = int(bbox.find(\"ymin\").text)\n",
    "        xmax = int(bbox.find(\"xmax\").text)\n",
    "        ymax = int(bbox.find(\"ymax\").text)\n",
    "\n",
    "        img_name_list.append(img_name)\n",
    "        width_list.append(width)\n",
    "        height_list.append(height)\n",
    "        x_min.append(xmin)\n",
    "        y_min.append(ymin)\n",
    "        x_max.append(xmax)\n",
    "        y_max.append(ymax)\n",
    "        label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>road0.png</td>\n",
       "      <td>267</td>\n",
       "      <td>400</td>\n",
       "      <td>98</td>\n",
       "      <td>62</td>\n",
       "      <td>208</td>\n",
       "      <td>232</td>\n",
       "      <td>trafficlight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>road1.png</td>\n",
       "      <td>400</td>\n",
       "      <td>283</td>\n",
       "      <td>154</td>\n",
       "      <td>63</td>\n",
       "      <td>258</td>\n",
       "      <td>281</td>\n",
       "      <td>trafficlight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>road10.png</td>\n",
       "      <td>400</td>\n",
       "      <td>267</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>244</td>\n",
       "      <td>263</td>\n",
       "      <td>trafficlight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>road100.png</td>\n",
       "      <td>400</td>\n",
       "      <td>385</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>363</td>\n",
       "      <td>326</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>road101.png</td>\n",
       "      <td>400</td>\n",
       "      <td>200</td>\n",
       "      <td>195</td>\n",
       "      <td>7</td>\n",
       "      <td>392</td>\n",
       "      <td>194</td>\n",
       "      <td>speedlimit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      img_name  width  height  x_min  y_min  x_max  y_max         label\n",
       "0    road0.png    267     400     98     62    208    232  trafficlight\n",
       "1    road1.png    400     283    154     63    258    281  trafficlight\n",
       "2   road10.png    400     267    106      3    244    263  trafficlight\n",
       "3  road100.png    400     385     35      5    363    326    speedlimit\n",
       "4  road101.png    400     200    195      7    392    194    speedlimit"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame(\n",
    "    {\n",
    "        \"img_name\": img_name_list,\n",
    "        \"width\": width_list,\n",
    "        \"height\": height_list,\n",
    "        \"x_min\": x_min,\n",
    "        \"y_min\": y_min,\n",
    "        \"x_max\": x_max,\n",
    "        \"y_max\": y_max,\n",
    "        \"label\": label_list,\n",
    "    }\n",
    ")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trafficlight': 0, 'speedlimit': 1, 'crosswalk': 2, 'stop': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = dataset[\"label\"].unique().tolist()\n",
    "class_training = {idx: label for idx, label in enumerate(classes)}\n",
    "classes = {label: idx for idx, label in enumerate(classes)}\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"class\"] = dataset[\"label\"].map(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>road0.png</td>\n",
       "      <td>267</td>\n",
       "      <td>400</td>\n",
       "      <td>98</td>\n",
       "      <td>62</td>\n",
       "      <td>208</td>\n",
       "      <td>232</td>\n",
       "      <td>trafficlight</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>road1.png</td>\n",
       "      <td>400</td>\n",
       "      <td>283</td>\n",
       "      <td>154</td>\n",
       "      <td>63</td>\n",
       "      <td>258</td>\n",
       "      <td>281</td>\n",
       "      <td>trafficlight</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>road10.png</td>\n",
       "      <td>400</td>\n",
       "      <td>267</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>244</td>\n",
       "      <td>263</td>\n",
       "      <td>trafficlight</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>road100.png</td>\n",
       "      <td>400</td>\n",
       "      <td>385</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>363</td>\n",
       "      <td>326</td>\n",
       "      <td>speedlimit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>road101.png</td>\n",
       "      <td>400</td>\n",
       "      <td>200</td>\n",
       "      <td>195</td>\n",
       "      <td>7</td>\n",
       "      <td>392</td>\n",
       "      <td>194</td>\n",
       "      <td>speedlimit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      img_name  width  height  x_min  y_min  x_max  y_max         label  class\n",
       "0    road0.png    267     400     98     62    208    232  trafficlight      0\n",
       "1    road1.png    400     283    154     63    258    281  trafficlight      0\n",
       "2   road10.png    400     267    106      3    244    263  trafficlight      0\n",
       "3  road100.png    400     385     35      5    363    326    speedlimit      1\n",
       "4  road101.png    400     200    195      7    392    194    speedlimit      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1244it [00:00, 8987.85it/s]\n"
     ]
    }
   ],
   "source": [
    "imgs = defaultdict(list)\n",
    "for idx, dt in tqdm(dataset.iterrows()):\n",
    "    sample_label_list = []\n",
    "    img_name = dt[\"img_name\"]\n",
    "    xmin = int(dt[\"x_min\"])\n",
    "    ymin = int(dt[\"y_min\"])\n",
    "    xmax = int(dt[\"x_max\"])\n",
    "    ymax = int(dt[\"y_max\"])\n",
    "    class_num = dt[\"class\"]\n",
    "    W, H = int(dt[\"width\"]), int(dt[\"height\"])\n",
    "\n",
    "    voc_bbox = (int(xmin), int(ymin), int(xmax), int(ymax))\n",
    "\n",
    "    x_center, y_center, w, h = pbx.convert_bbox(\n",
    "        voc_bbox, from_type=\"voc\", to_type=\"yolo\", image_size=(W, H)\n",
    "    )\n",
    "\n",
    "    sample_label_list.append(str(class_num))\n",
    "    sample_label_list.append(str(x_center))\n",
    "    sample_label_list.append(str(y_center))\n",
    "    sample_label_list.append(str(w))\n",
    "    sample_label_list.append(str(h))\n",
    "    line = \" \".join(sample_label_list)\n",
    "\n",
    "    imgs[img_name].append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dir = \"../data/labels\"\n",
    "if os.path.exists(labels_dir):\n",
    "    shutil.rmtree(labels_dir)\n",
    "os.mkdir(labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name, lines in imgs.items():\n",
    "    img_name = img_name.split('.')[0]\n",
    "    with open(f'{labels_dir}/{img_name}.txt', 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../data/train/\"\n",
    "val_dir = \"../data/val\"\n",
    "labels_path = \"../data/labels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(train_dir):\n",
    "    shutil.rmtree(train_dir)\n",
    "\n",
    "if os.path.exists(val_dir):\n",
    "    shutil.rmtree(val_dir)\n",
    "\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(val_dir)\n",
    "\n",
    "# train, val each containing images and labels folders\n",
    "os.mkdir(train_dir + \"/images\")\n",
    "os.mkdir(train_dir + \"/labels\")\n",
    "os.mkdir(val_dir + \"/images\")\n",
    "os.mkdir(val_dir + \"/labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(images_path)\n",
    "shuffle(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(files, ratio):\n",
    "    elements = len(files)\n",
    "    middle = int(elements * ratio)\n",
    "    return [files[:middle], files[middle:]]\n",
    "\n",
    "def copy_files(images_path, labels_path, destination_path, files):\n",
    "    \n",
    "    for file_name in files:\n",
    "        file_name = file_name.split('.')[0]\n",
    "\n",
    "        src = images_path + f'/{file_name}.png'\n",
    "        dst = destination_path + '/images'\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "        src = labels_path + f'/{file_name}.txt'\n",
    "        dst = destination_path + '/labels'\n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.75\n",
    "train_files, val_files = split(files, train_ratio)\n",
    "\n",
    "root = 'data/traffic_sign_data'\n",
    "\n",
    "copy_files(images_path, labels_path, train_dir, train_files)\n",
    "copy_files(images_path, labels_path, val_dir, val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/sign_data.yaml', 'w') as f:\n",
    "    f.write('train: ../../data/train/images\\n')\n",
    "    f.write('val: ../../data/val/images\\n')\n",
    "    f.write('nc: 4\\n')\n",
    "    f.write(f\"names: {class_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.234 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.233 🚀 Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=../data/sign_data.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=320, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train3\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011628 parameters, 3011612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ADMIN\\OneDrive - EPITA\\Project-Demo\\road-sign-detection\\data\\train\\labels... 657 images, 0 backgrounds, 0 corrupt: 100%|██████████| 657/657 [00:00<00:00, 897.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\ADMIN\\OneDrive - EPITA\\Project-Demo\\road-sign-detection\\data\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ADMIN\\OneDrive - EPITA\\Project-Demo\\road-sign-detection\\data\\val\\labels... 220 images, 0 backgrounds, 0 corrupt: 100%|██████████| 220/220 [00:00<00:00, 791.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\ADMIN\\OneDrive - EPITA\\Project-Demo\\road-sign-detection\\data\\val\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G     0.8801      2.445     0.9015          1        320: 100%|██████████| 42/42 [00:32<00:00,  1.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300      0.517      0.394      0.455      0.328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G     0.8222      1.284     0.8718          2        320: 100%|██████████| 42/42 [00:31<00:00,  1.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300      0.855      0.486      0.737      0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      0.834      1.168     0.8938          1        320: 100%|██████████| 42/42 [00:30<00:00,  1.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300      0.786      0.673      0.787      0.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G     0.7923      1.023     0.8814          2        320: 100%|██████████| 42/42 [00:34<00:00,  1.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300      0.747      0.694       0.67      0.511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G     0.7794     0.9266     0.8757          2        320: 100%|██████████| 42/42 [00:33<00:00,  1.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300      0.799      0.747      0.761      0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G     0.7119      0.829      0.852          1        320: 100%|██████████| 42/42 [00:31<00:00,  1.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300      0.907      0.768       0.84      0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G     0.6928     0.7791     0.8645          1        320: 100%|██████████| 42/42 [00:30<00:00,  1.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300       0.88      0.755      0.845      0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G     0.6848     0.7255     0.8579          3        320: 100%|██████████| 42/42 [00:31<00:00,  1.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300      0.889      0.791       0.85      0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G     0.6902     0.7116      0.839          2        320: 100%|██████████| 42/42 [00:31<00:00,  1.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300      0.926      0.781      0.868      0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G     0.6434     0.6606     0.8211          1        320: 100%|██████████| 42/42 [00:34<00:00,  1.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300      0.869      0.812      0.871      0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.103 hours.\n",
      "Optimizer stripped from runs\\detect\\train3\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train3\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train3\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.233 🚀 Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300      0.873      0.804      0.872      0.717\n",
      "          trafficlight        220         36      0.684      0.556       0.68      0.467\n",
      "            speedlimit        220        206      0.999      0.956      0.984       0.83\n",
      "             crosswalk        220         39      0.914      0.795      0.849      0.689\n",
      "                  stop        220         19      0.896       0.91      0.975      0.884\n",
      "Speed: 0.2ms preprocess, 12.2ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train3\u001b[0m\n",
      "Ultralytics YOLOv8.0.233 🚀 Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ADMIN\\OneDrive - EPITA\\Project-Demo\\road-sign-detection\\data\\val\\labels.cache... 220 images, 0 backgrounds, 0 corrupt: 100%|██████████| 220/220 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:04<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        220        300      0.873      0.804      0.872      0.717\n",
      "          trafficlight        220         36      0.684      0.556      0.678      0.466\n",
      "            speedlimit        220        206      0.999      0.956      0.984       0.83\n",
      "             crosswalk        220         39      0.914      0.795      0.849      0.689\n",
      "                  stop        220         19      0.896      0.909      0.975      0.883\n",
      "Speed: 0.2ms preprocess, 11.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train32\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n.pt\")\n",
    "results = model.train(\n",
    "    data=\"../data/sign_data.yaml\", epochs=10, imgsz=320\n",
    ")  # train the model\n",
    "results = model.val()  # evaluate model performance on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
